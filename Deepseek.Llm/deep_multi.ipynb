{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f0d6d9-41d2-4e40-a87f-e31a8fafd156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daveee\\AppData\\Local\\Temp\\ipykernel_11340\\3111658349.py:16: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Daveee\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not a ZIP archive (so not a DOCX file): 'Documents\\\\~$faqs.docx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Run processing if FAISS DB doesn't exist\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss_faqs_db\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(process_documents())\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ FAISS index already exists. Skipping processing.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 45\u001b[0m, in \u001b[0;36mprocess_documents\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     44\u001b[0m     loader \u001b[38;5;241m=\u001b[39m UnstructuredWordDocumentLoader(filepath)\n\u001b[1;32m---> 45\u001b[0m     loaded_docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m loaded_docs:\n\u001b[0;32m     47\u001b[0m         lang \u001b[38;5;241m=\u001b[39m detect(doc\u001b[38;5;241m.\u001b[39mpage_content) \u001b[38;5;28;01mif\u001b[39;00m doc\u001b[38;5;241m.\u001b[39mpage_content\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:32\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:107\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_elements()\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\word_document.py:139\u001b[0m, in \u001b[0;36mUnstructuredWordDocumentLoader._get_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_docx\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m partition_docx(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munstructured_kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\unstructured\\partition\\common\\metadata.py:162\u001b[0m, in \u001b[0;36mapply_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 162\u001b[0m     elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# unique-ify elements\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;66;03m# instance).\u001b[39;00m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# ------------------------------------------------------------------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m elements \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[0;32m     77\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\unstructured\\partition\\docx.py:137\u001b[0m, in \u001b[0;36mpartition_docx\u001b[1;34m(filename, file, include_page_breaks, infer_table_structure, starting_page_number, strategy, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;129m@apply_metadata\u001b[39m(FileType\u001b[38;5;241m.\u001b[39mDOCX)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;129m@add_chunking_strategy\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpartition_docx\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[0;32m    111\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Partitions Microsoft Word Documents in .docx format into its document elements.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        there.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     opts \u001b[38;5;241m=\u001b[39m DocxPartitionerOptions\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    138\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[0;32m    139\u001b[0m         file_path\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    140\u001b[0m         include_page_breaks\u001b[38;5;241m=\u001b[39minclude_page_breaks,\n\u001b[0;32m    141\u001b[0m         infer_table_structure\u001b[38;5;241m=\u001b[39minfer_table_structure,\n\u001b[0;32m    142\u001b[0m         starting_page_number\u001b[38;5;241m=\u001b[39mstarting_page_number,\n\u001b[0;32m    143\u001b[0m         strategy\u001b[38;5;241m=\u001b[39mstrategy,\n\u001b[0;32m    144\u001b[0m     )\n\u001b[0;32m    146\u001b[0m     elements \u001b[38;5;241m=\u001b[39m _DocxPartitioner\u001b[38;5;241m.\u001b[39miter_document_elements(opts)\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(elements)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\unstructured\\partition\\docx.py:185\u001b[0m, in \u001b[0;36mDocxPartitionerOptions.load\u001b[1;34m(cls, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DocxPartitionerOptions:\n\u001b[0;32m    184\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct and validate an instance.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39m_validate()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\unstructured\\partition\\docx.py:326\u001b[0m, in \u001b[0;36mDocxPartitionerOptions._validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno such file or directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mis_zipfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path):\n\u001b[1;32m--> 326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot a ZIP archive (so not a DOCX file): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file:\n\u001b[0;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mis_zipfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file):\n",
      "\u001b[1;31mValueError\u001b[0m: not a ZIP archive (so not a DOCX file): 'Documents\\\\~$faqs.docx'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langdetect import detect, DetectorFactory\n",
    "\n",
    "# Ensure consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "DOCS_DIR = \"Documents\"  # Folder containing PDFs, DOCX, and image files\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        lang = detect(text) if text.strip() else \"unknown\"\n",
    "        return Document(page_content=text, metadata={\"source\": image_path, \"language\": lang})\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_documents():\n",
    "    docs = []\n",
    "\n",
    "    for filename in os.listdir(DOCS_DIR):\n",
    "        filepath = os.path.join(DOCS_DIR, filename)\n",
    "\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            loaded_docs = loader.load()\n",
    "            # Add language detection metadata\n",
    "            for doc in loaded_docs:\n",
    "                lang = detect(doc.page_content) if doc.page_content.strip() else \"unknown\"\n",
    "                doc.metadata[\"language\"] = lang\n",
    "            docs.extend(loaded_docs)\n",
    "\n",
    "        elif filename.endswith(\".docx\"):\n",
    "            loader = UnstructuredWordDocumentLoader(filepath)\n",
    "            loaded_docs = loader.load()\n",
    "            for doc in loaded_docs:\n",
    "                lang = detect(doc.page_content) if doc.page_content.strip() else \"unknown\"\n",
    "                doc.metadata[\"language\"] = lang\n",
    "            docs.extend(loaded_docs)\n",
    "\n",
    "        elif filename.endswith((\".png\", \".jpg\", \".jpeg\", \".tif\")):\n",
    "            doc = extract_text_from_image(filepath)\n",
    "            if doc:\n",
    "                docs.append(doc)\n",
    "        else:\n",
    "            continue  # skip unsupported formats\n",
    "\n",
    "    # Split text into chunks (this will preserve metadata)\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    # Debug: show language distribution\n",
    "    lang_counts = {}\n",
    "    for c in chunks:\n",
    "        lang = c.metadata.get(\"language\", \"unknown\")\n",
    "        lang_counts[lang] = lang_counts.get(lang, 0) + 1\n",
    "    print(f\"Language counts in chunks: {lang_counts}\")\n",
    "\n",
    "    # Extract texts for embedding\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "\n",
    "    # Create FAISS index with all language data\n",
    "    vector_store = FAISS.from_documents(chunks, embedding_model)\n",
    "    vector_store.save_local(\"faiss_faqs_db\")\n",
    "\n",
    "    return f\"✅ {len(texts)} chunks processed and stored in FAISS!\"\n",
    "\n",
    "# Run processing if FAISS DB doesn't exist\n",
    "if not os.path.exists(\"faiss_faqs_db\"):\n",
    "    print(process_documents())\n",
    "else:\n",
    "    print(\"✅ FAISS index already exists. Skipping processing.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2c012-007f-4366-b404-e1c327b5ee21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
